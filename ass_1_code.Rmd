---
title: "Assignment 1"
author: "Andreas"
date: "2024-02-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Assignment 1 ##


```{r}
# pckages
library(tidyverse)
library(patchwork)
set.seed(1983)
```


First, we implement the random agent and the WSLS agent as functions

```{r}
RandomAgent_f <- function(rate){
  choice <- rbinom(1, 1, rate)
  return(choice)
}

WSLSAgent_f <- function(prevChoice, Feedback){
  if (Feedback == 1) {
    choice = prevChoice
  } else if (Feedback == 0) {
    choice = 1 - prevChoice
  }
  return(choice)
}
```

Environment of the games
```{r}
trials = 120
agents = 100
```



### Implementing the two strategies we selected as functions ###



The WSLS strategy with a probabilistic decision mechanism.
We implement a weight parameter for both wins and losses.

```{r}
WSLSProbAgent_f <- function(prevChoice, Feedback, weight_win, weight_lose){
  if (Feedback == 1) {
    choice = ifelse(rbinom(1,1,weight_win) == 1,  prevChoice,  1 - prevChoice)
    
  } else if (Feedback == 0) {
    choice = ifelse(rbinom(1,1,weight_lose) == 1,  1 - prevChoice, prevChoice)
  }
  return(choice)
}


```


Reinforcement-learning strategy.
Implement learning rate, a.

```{r}

RLAgent_f <- function(prevChoice, Feedback, prev_EV, a){
  
  others_prevChoice = ifelse(Feedback == 1, prevChoice, 1 - prevChoice)
  
  PE = others_prevChoice - prev_EV
  
  EV = prev_EV + a * PE
  
  choice = ifelse(EV > 0.5, 1, ifelse(EV == 0.5, rbinom(1,1,0.5), 0))
  
  return(list(choice = choice, EV = EV))
}
```




## Letting agents play against each other


WSLS Probabilistic agent playing the WSLS agent

```{r}
# Against a Win-Stay-Lose Shift
Self <- rep(NA, trials)
Other <- rep(NA, trials)

Self[1] <- RandomAgent_f(0.5)
Other[1] <- RandomAgent_f(0.5)

weight_win = 0.7
weight_lose = 0.9

for (i in 2:trials) {
  if (Self[i - 1] == Other[i - 1]) {
    Feedback = 1
  } else {Feedback = 0}
  Self[i] <- WSLSProbAgent_f(Self[i - 1], Feedback, weight_win, weight_lose)
  Other[i] <- WSLSAgent_f(Other[i - 1], 1 - Feedback)
}

sum(Self == Other)

# Plotting behaviour for a set 
df <- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other))

ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, Self)) +
  geom_line(color = "blue", aes(trial, Other))

# Plotting Performance
df$cumulativerateSelf <- cumsum(df$Feedback) / seq_along(df$Feedback)
df$cumulativerateOther <- cumsum(1 - df$Feedback) / seq_along(df$Feedback)

ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, cumulativerateSelf)) +
  geom_line(color = "blue", aes(trial, cumulativerateOther)) +
  ggtitle("Probabilistic WSLS (red) vs WSLS (blue)")
```



A numerical experiment altering the weights systematically.
The lowest weight we set is 0.5 indicating random behavior. Weight lower than 0.5 indicates a probabilistic win-shift lose-stay strategy.
Each combination of weights is run with 100 agents.

```{r}
df1 = NULL

for(weight_win in seq(from = 0.5, to = 1, by = 0.1)){
  
  for(weight_lose in seq(from = 0.5, to = 1, by = 0.1)){
    
    for(agent in seq(agents)){
      
      Self <- rep(NA, trials)
      Other <- rep(NA, trials)
      
      Self[1] <- RandomAgent_f(0.5)
      Other[1] <- RandomAgent_f(0.5)
      
      for (i in 2:trials) {
        if (Self[i - 1] == Other[i - 1]) {
          Feedback = 1
        } else {Feedback = 0}
        Self[i] <- WSLSProbAgent_f(Self[i - 1], Feedback, weight_win, weight_lose)
        Other[i] <- WSLSAgent_f(Other[i - 1], 1 - Feedback)
        
        
      }
      
      temp <- tibble(Self, Other, trial = seq(trials), agent,Feedback = as.numeric(Self == Other),
                     weight_win, weight_lose)
    
      if (exists("df1")) {df1 = rbind(df1, temp) } else {df1 = temp}
      
    }
    
  }
  
}
```



Visualizing performance (feedback)

Maybe take difference between weight_win and weight_lose or ratio between them to have one variable to group by.

For now, we facet-wrap by one weight and color by the other:

```{r}
p1 = ggplot(df1, aes(trial, Feedback, group = weight_win, color = weight_win)) +
  geom_smooth(se = F) + facet_wrap(.~weight_lose) + theme_classic()

p1 + scale_color_gradient(low = "blue", high = "orange")
```

What do we see? The optimal strategy (when playing against a pure WSLS agent) is to certainly shift when losing, i.e. having a high weight_lose but randomly choose left or right when winning, i.e. having a low weight_win.




Letting the RL agent play a biased random agent

```{r}

# RL against random biased

# empty arrays to fill
Self <- rep(NA, trials)
Other <- rep(NA, trials)
EV <- rep(NA, trials)

# bias for the random agents
rate <- 0.75

#learning rate
a <- 0.05

# First round is random for the RL agent, and expected value is "neutral"
Self[1] <- RandomAgent_f(0.5)
Other[1] <- RandomAgent_f(rate)
EV[1] <- 0.5


for (i in 2:trials) {
  if (Self[i - 1] == Other[i - 1]) {
    Feedback = 1
  } else {Feedback = 0}
  temp <- RLAgent_f(prevChoice = Self[i - 1], Feedback, EV[i - 1], a)
  Self[i] <- temp$choice
  EV[i] <- temp$EV
  Other[i] <- RandomAgent_f(rate)
}

sum(Self == Other)

# Plotting behaviour for a set 
df <- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other))

ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, Self)) +
  geom_line(color = "blue", aes(trial, Other))

# Plotting Performance
df$cumulativerateSelf <- cumsum(df$Feedback) / seq_along(df$Feedback)
df$cumulativerateOther <- cumsum(1 - df$Feedback) / seq_along(df$Feedback)

ggplot(df) + theme_classic() +
  geom_line(color = "red", aes(trial, cumulativerateSelf)) +
  geom_line(color = "blue", aes(trial, cumulativerateOther)) +
  ggtitle("RL (red) vs random biased (blue)")


plot(df$trial, EV)
```

RL playing Random

A numerical experiment altering the bias and learning rate systematically.
The lowest bias we set is 0.5 indicating random behavior.
Each combination of weights is run with 100 agents.

```{r}
df2 = NULL

for(rate in seq(from = 0.5, to = 1, by = 0.1)){
  
  for(a in seq(from = 0.02, to = 0.08, by = 0.01)){
    
    for(agent in seq(agents)){
      
      Self <- rep(NA, trials)
      Other <- rep(NA, trials)
      EV <- rep(NA, trials)
      
      Self[1] <- RandomAgent_f(0.5)
      Other[1] <- RandomAgent_f(rate)
      EV[1] <- 0.5
      
      for (i in 2:trials) {
        if (Self[i - 1] == Other[i - 1]) {
          Feedback = 1
        } else {Feedback = 0}
        
        result <- RLAgent_f(Self[i - 1], Feedback, EV[i - 1], a)
        
        Self[i] <- result$choice
        EV[i] <- result$EV
        Other[i] <- RandomAgent_f(rate)
        
        
      }
      
      temp <- tibble(Self, Other, EV, trial = seq(trials), agent,Feedback = as.numeric(Self == Other), rate, a)
    
      if (exists("df2")) {df2 = rbind(df2, temp) } else {df2 = temp}
      
    }
    
  }
  
}
```

```{r}
write.csv(df2, "RL_vs_Random.csv")
```



Visualizing performance (feedback)

Maybe take difference between weight_win and weight_lose or ratio between them to have one variable to group by.

For now, we facet-wrap bias and color by learning rate:

```{r}
p2 = ggplot(df2, aes(trial, Feedback, group = a, color = a)) +
  geom_smooth(se = F) + facet_wrap(.~rate) + theme_classic()

p2 + scale_color_gradient(low = "blue", high = "orange")
```


Plot the EV instead of a smoothed feedback

```{r}
p3 = ggplot(df2, aes(trial, EV, group = a, color = a)) +
  geom_smooth(se = F) +
  #geom_point(size = 0.1) +
  facet_wrap(.~rate) + theme_classic() + ggtitle("EV over trials")

p3 + scale_color_gradient(low = "blue", high = "orange")

```




